{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating BERT Embeddings\n",
    "\n",
    "As we found from our exploratory data analysis, the textual content and raw sentiment of the reviews is indicative of its corresponding rating. However, we found out that we cannot simply use raw sentiment as training data as it does not capture any contextual information regarding the review. Therefore, we must find a way to encode our data in a way which captures both sentiment and context.\n",
    "\n",
    "We can create this encoding by leveraging the power of transfer learning and using a pre-trained SOTA deep neural network model: BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "\n",
    "# BERT Specific Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub \n",
    "import tensorflow_text as text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a pretrained BERT model provided by the Tensorflow module. More specifically, we use a variant of a pre-trained BERT model called BERT Experts which has been pre-trained on the Stanford Sentiment Treebank (SST-2) dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing layer to generate the tokenized sentences and input mask\n",
    "bert_preprocess = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "# Encoder layer which generates word-level and setence-level 768-dimensional text embeddings \n",
    "bert = hub.KerasLayer('https://tfhub.dev/google/experts/bert/wiki_books/sst2/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UNINSPIRED</td>\n",
       "      <td>This album is a travesty to the songs of the 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mastiff Aristocratic Guardian by DeeDee Andersson</td>\n",
       "      <td>I found this book a complete waist of time, I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Scratches. Turns off by itself. Not working.</td>\n",
       "      <td>The product I got had scratches on its surface...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Worst College Book of all time</td>\n",
       "      <td>Ok well it may not be the worst book that I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>just okay</td>\n",
       "      <td>It was ok. Could of gotten into the other char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product, no problems</td>\n",
       "      <td>Good little memory stick. Currently using as m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5</td>\n",
       "      <td>Beautiful</td>\n",
       "      <td>How anyone can write such fun tropical songs a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5</td>\n",
       "      <td>Is that thing loaded???</td>\n",
       "      <td>Sure, its one of The Great Man's best movies, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>5</td>\n",
       "      <td>WOW!</td>\n",
       "      <td>I finally bought and watched this classic epic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>5</td>\n",
       "      <td>HILLERMAN AND THE NTP</td>\n",
       "      <td>I'm a little late in posting a review, one whi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rating                                              Title  \\\n",
       "0           1                                         UNINSPIRED   \n",
       "1           1  Mastiff Aristocratic Guardian by DeeDee Andersson   \n",
       "2           1       Scratches. Turns off by itself. Not working.   \n",
       "3           1                     Worst College Book of all time   \n",
       "4           1                                          just okay   \n",
       "...       ...                                                ...   \n",
       "49995       5                         Great product, no problems   \n",
       "49996       5                                          Beautiful   \n",
       "49997       5                            Is that thing loaded???   \n",
       "49998       5                                               WOW!   \n",
       "49999       5                              HILLERMAN AND THE NTP   \n",
       "\n",
       "                                                  Review  \n",
       "0      This album is a travesty to the songs of the 5...  \n",
       "1      I found this book a complete waist of time, I ...  \n",
       "2      The product I got had scratches on its surface...  \n",
       "3      Ok well it may not be the worst book that I ha...  \n",
       "4      It was ok. Could of gotten into the other char...  \n",
       "...                                                  ...  \n",
       "49995  Good little memory stick. Currently using as m...  \n",
       "49996  How anyone can write such fun tropical songs a...  \n",
       "49997  Sure, its one of The Great Man's best movies, ...  \n",
       "49998  I finally bought and watched this classic epic...  \n",
       "49999  I'm a little late in posting a review, one whi...  \n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the downsampled data\n",
    "data_dir = \"data\"\n",
    "data = pd.read_csv(join(data_dir, \"downsampled_train_50000.csv\"), names=['Rating', 'Title', 'Review'])\n",
    "data = data.iloc[1:, :].reset_index(drop=True)\n",
    "data[\"Rating\"] = data[\"Rating\"].apply(int) \n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (7200,) | X_val: (1800,) | X_test: (1000,) | \n",
      "y_train: (7200,) | y_val: (1800,) | y_test: (1000,) | \n"
     ]
    }
   ],
   "source": [
    "# Create train, val, and test sets\n",
    "train_data = shuffle(data)[:10000]\n",
    "X = train_data[\"Review\"].to_numpy()\n",
    "y = train_data[\"Rating\"].to_numpy() - 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(f\"X_train: {X_train.shape} | X_val: {X_val.shape} | X_test: {X_test.shape} | \\n\" +\n",
    "    f\"y_train: {y_train.shape} | y_val: {y_val.shape} | y_test: {y_test.shape} | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [14:10<00:00,  8.50s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 72, 768), dtype=float32, numpy=\n",
       "array([[[ 2.5606203e-01,  2.2648114e-01, -7.3399884e-01, ...,\n",
       "         -1.5405677e-01, -7.4479479e-01, -2.3880896e-01],\n",
       "        [ 6.0433829e-01, -3.5289210e-01, -3.2896611e-01, ...,\n",
       "         -7.0303285e-01, -9.6206862e-01,  3.5354868e-04],\n",
       "        [-2.0405006e-01, -8.8491130e-01,  3.6593831e-01, ...,\n",
       "          2.5963566e-01, -5.5853081e-01,  1.7557411e-01],\n",
       "        ...,\n",
       "        [ 1.1836937e-01,  6.7327064e-01,  6.7677331e-01, ...,\n",
       "          5.5701458e-01,  7.0627445e-01, -6.5629554e-01],\n",
       "        [ 1.5376495e-01, -2.7541134e-01,  8.3992398e-01, ...,\n",
       "          1.3801001e-01, -6.7640889e-01, -6.8531263e-01],\n",
       "        [ 7.9678363e-01, -8.7943457e-02,  4.2670679e-01, ...,\n",
       "          6.2972367e-01,  8.2276469e-01, -5.8434826e-01]],\n",
       "\n",
       "       [[ 2.4091324e-01, -3.3776504e-01,  6.5796727e-01, ...,\n",
       "         -9.6181311e-02,  4.7297868e-01, -6.9508857e-01],\n",
       "        [ 6.0087836e-01, -2.8265351e-01, -2.1154387e-01, ...,\n",
       "         -1.7967005e-01, -7.7156085e-01,  3.1094468e-01],\n",
       "        [-1.1487578e-01,  2.4746935e-01, -3.6428225e-01, ...,\n",
       "         -5.2392566e-01, -9.3805945e-01,  3.0873784e-01],\n",
       "        ...,\n",
       "        [ 6.8265092e-01, -4.3903121e-01, -1.4990947e-01, ...,\n",
       "         -7.8382927e-01, -8.8663000e-01,  1.6034739e-01],\n",
       "        [ 4.4606808e-01,  4.8251045e-01,  7.4580443e-01, ...,\n",
       "          7.0321238e-01,  6.5500045e-01, -4.6968752e-01],\n",
       "        [ 1.8614508e-01,  2.5561893e-01, -1.2075501e-01, ...,\n",
       "         -5.5746514e-01, -9.0799081e-01, -5.3115908e-02]],\n",
       "\n",
       "       [[ 4.8476908e-01,  3.8742417e-01,  7.1044463e-01, ...,\n",
       "          3.9800519e-01,  7.9607600e-01, -2.7717647e-01],\n",
       "        [ 4.6484309e-01, -2.9607412e-01, -6.2239188e-01, ...,\n",
       "         -7.7044487e-01, -8.7507719e-01,  7.7373403e-01],\n",
       "        [ 5.9657162e-01,  4.0241003e-01, -1.6426803e-01, ...,\n",
       "         -6.6512412e-01, -9.6583146e-01,  3.9876503e-01],\n",
       "        ...,\n",
       "        [ 4.1346937e-01,  1.4547795e-01,  5.3928804e-01, ...,\n",
       "          5.7776207e-01,  3.5218564e-01, -3.8885379e-01],\n",
       "        [ 5.3901911e-01, -2.9891416e-01, -2.4942352e-01, ...,\n",
       "         -4.4792491e-01, -9.2795718e-01,  5.7887363e-01],\n",
       "        [ 6.7326695e-02, -2.9526111e-02, -2.9297784e-01, ...,\n",
       "         -8.1620181e-01, -9.6222675e-01,  2.3776674e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.3651426e-01, -6.5133470e-01, -3.0597591e-01, ...,\n",
       "         -3.6976540e-01, -9.3163610e-01,  7.9978359e-01],\n",
       "        [-1.5286826e-01, -3.5531986e-01,  5.9053481e-01, ...,\n",
       "          6.0244840e-01,  6.3657516e-01,  9.8654248e-02],\n",
       "        [ 7.0994861e-02, -3.1054291e-01,  3.3211365e-01, ...,\n",
       "          6.3683701e-01, -3.2230410e-01,  2.4465489e-01],\n",
       "        ...,\n",
       "        [ 2.7512308e-02, -3.6676612e-01,  2.6654282e-01, ...,\n",
       "          6.1062169e-01, -6.5231061e-01, -3.8950956e-01],\n",
       "        [ 2.6260209e-01,  1.5028702e-03, -5.5421239e-01, ...,\n",
       "         -7.7541149e-01, -9.7519451e-01,  5.5055797e-01],\n",
       "        [ 1.4545593e-01, -8.6430246e-01,  1.1750155e-03, ...,\n",
       "         -2.9815593e-01, -5.7674444e-01,  1.0825489e-01]],\n",
       "\n",
       "       [[-3.3558387e-01, -2.6624152e-01,  4.0355399e-03, ...,\n",
       "          6.3225448e-01,  6.2670559e-01, -8.0144757e-01],\n",
       "        [-1.8772916e-01, -3.5903180e-01,  4.0135139e-01, ...,\n",
       "          7.4131244e-01, -3.0215737e-01, -5.2889597e-01],\n",
       "        [ 3.3496132e-01,  2.5431198e-01, -5.9606117e-01, ...,\n",
       "         -6.4832926e-01, -8.2367134e-01,  6.3122582e-01],\n",
       "        ...,\n",
       "        [ 2.2853799e-01, -4.9742259e-02,  1.4234044e-01, ...,\n",
       "         -8.5470551e-01, -6.8524456e-01, -1.5437187e-01],\n",
       "        [ 1.3329819e-01,  3.2251081e-01, -6.4495873e-01, ...,\n",
       "         -5.3301185e-01, -8.3696818e-01,  5.6333959e-01],\n",
       "        [ 4.4322935e-01, -4.0627050e-01, -3.0451244e-01, ...,\n",
       "         -4.5909256e-01, -9.2641240e-01,  1.7452361e-01]],\n",
       "\n",
       "       [[-2.3922468e-02,  2.4501394e-02,  5.0537062e-01, ...,\n",
       "          7.3195302e-01,  4.0075310e-02, -7.1697742e-01],\n",
       "        [ 1.6366436e-01,  2.2838286e-01, -3.0763781e-01, ...,\n",
       "         -8.4346443e-01, -9.1431272e-01,  7.7658784e-01],\n",
       "        [-1.9013616e-01,  5.8718455e-01,  4.7913939e-01, ...,\n",
       "          7.7341038e-01, -8.1674345e-02, -6.4695746e-01],\n",
       "        ...,\n",
       "        [ 1.5546907e-01, -4.2975950e-01,  2.4386007e-01, ...,\n",
       "         -2.2681960e-01, -2.6913026e-01,  3.0646265e-01],\n",
       "        [ 1.7119107e-01,  2.7537456e-02, -5.2893049e-01, ...,\n",
       "         -6.2705129e-01, -9.5405781e-01,  6.2760830e-01],\n",
       "        [ 4.0816230e-01, -1.9921787e-01,  3.6424574e-01, ...,\n",
       "         -8.5357088e-01, -9.8135549e-01,  2.7849790e-01]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate train embeddings\n",
    "def generate_bert_embeddings(data):\n",
    "    \"\"\"Generate the BERT embeddings for a given Series/list of senetences\"\"\"\n",
    "    return bert(bert_preprocess(data))['pooled_output'] \n",
    "\n",
    "def generate_embeddings_list(data):\n",
    "    \"\"\"Generate embeddings of parts of list individually and then concatenate. \n",
    "    Create to overcome performance issues.\"\"\"\n",
    "    factor = int(data.shape[0]/100)\n",
    "    embeddings_list = []\n",
    "    for i in tqdm(range(0, 100)):\n",
    "        embeddings_list.append(generate_bert_embeddings(X[factor*i: factor*(i+1)]))\n",
    "    return embeddings_list\n",
    "    \n",
    "el = generate_embeddings_list(X_train)\n",
    "embeddings = tf.stack(el)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.reshape(embeddings, (7200, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train embeddings\n",
    "import pickle\n",
    "pickle.dump(embeddings, open(join(data_dir, \"downsampled_shuffled_train_embeddings.pkl\"), \"wb\"))\n",
    "pickle.dump(y_train, open(join(data_dir, \"downsampled_shuffled_train_labels.pkl\"), \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating val data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:20<00:00,  2.00s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:54<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate and save validation and test embeddings \n",
    "print(\"Generating val data...\")\n",
    "val_embeddings = tf.reshape(tf.stack(generate_embeddings_list(X_val)), (1800, 768))\n",
    "print(\"Generating test data...\")\n",
    "test_embeddings = tf.reshape(tf.stack(generate_embeddings_list(X_test)), (1000, 768))\n",
    "\n",
    "pickle.dump(val_embeddings, open(join(data_dir, \"downsampled_shuffled_val_embeddings.pkl\"), \"wb\"))\n",
    "pickle.dump(y_val, open(join(data_dir, \"downsampled_shuffled_val_labels.pkl\"), \"wb\"))\n",
    "\n",
    "pickle.dump(test_embeddings, open(join(data_dir, \"downsampled_shuffled_test_embeddings.pkl\"), \"wb\"))\n",
    "pickle.dump(y_test, open(join(data_dir, \"downsampled_shuffled_test_labels.pkl\"), \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('askoski': conda)",
   "language": "python",
   "name": "python37564bitaskoskicondaf89acb10eb5d40d0a2b1becabd7f59e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
